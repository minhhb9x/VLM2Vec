{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34cb357d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniforge3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DropoutAddRMSNorm of flash_attn is not installed!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniforge3/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/workspace/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n"
     ]
    }
   ],
   "source": [
    "from src.arguments import ModelArguments, DataArguments\n",
    "from src.model.model import MMEBModel\n",
    "from src.model.processor import load_processor, QWEN2_VL, VLM_IMAGE_TOKENS, Qwen2_VL_process_fn\n",
    "from src.utils.basic_utils import batch_to_device\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "\n",
    "from transformers.image_transforms import (\n",
    "    convert_to_rgb,\n",
    "    resize,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d66127b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-23 11:59:34,725] INFO [src.utils.basic_utils:21] Loading processor from: TIGER-Lab/VLM2Vec-Qwen2VL-2B\n",
      "[2025-12-23 11:59:34,728] DEBUG [urllib3.connectionpool:1049] Starting new HTTPS connection (1): huggingface.co:443\n",
      "[2025-12-23 11:59:35,096] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/preprocessor_config.json HTTP/1.1\" 307 0\n",
      "[2025-12-23 11:59:35,130] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e/preprocessor_config.json HTTP/1.1\" 200 0\n",
      "[2025-12-23 11:59:35,401] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/tokenizer_config.json HTTP/1.1\" 307 0\n",
      "[2025-12-23 11:59:35,435] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "[2025-12-23 11:59:36,239] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/preprocessor_config.json HTTP/1.1\" 307 0\n",
      "[2025-12-23 11:59:36,271] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e/preprocessor_config.json HTTP/1.1\" 200 0\n",
      "[2025-12-23 11:59:36,675] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/tokenizer_config.json HTTP/1.1\" 307 0\n",
      "[2025-12-23 11:59:36,709] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "[2025-12-23 11:59:38,200] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/processor_config.json HTTP/1.1\" 404 0\n",
      "[2025-12-23 11:59:38,438] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/chat_template.json HTTP/1.1\" 307 0\n",
      "[2025-12-23 11:59:38,472] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e/chat_template.json HTTP/1.1\" 200 0\n",
      "[2025-12-23 11:59:38,708] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/chat_template.jinja HTTP/1.1\" 404 0\n",
      "[2025-12-23 11:59:39,728] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/config.json HTTP/1.1\" 307 0\n",
      "[2025-12-23 11:59:39,761] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e/config.json HTTP/1.1\" 200 0\n",
      "[2025-12-23 11:59:39,766] INFO [src.utils.basic_utils:21] Loading backbone [qwen2_vl] from TIGER-Lab/VLM2Vec-Qwen2VL-2B\n",
      "[2025-12-23 11:59:39,994] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /Qwen/Qwen2-VL-2B-Instruct/resolve/main/config.json HTTP/1.1\" 307 0\n",
      "[2025-12-23 11:59:40,026] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/Qwen/Qwen2-VL-2B-Instruct/895c3a49bc3fa70a340399125c650a463535e71c/config.json HTTP/1.1\" 200 0\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.21it/s]\n",
      "[2025-12-23 11:59:41,035] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /Qwen/Qwen2-VL-2B-Instruct/resolve/main/generation_config.json HTTP/1.1\" 307 0\n",
      "[2025-12-23 11:59:41,067] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/Qwen/Qwen2-VL-2B-Instruct/895c3a49bc3fa70a340399125c650a463535e71c/generation_config.json HTTP/1.1\" 200 0\n",
      "[2025-12-23 11:59:41,071] INFO [src.utils.basic_utils:21] Loading LoRA from TIGER-Lab/VLM2Vec-Qwen2VL-2B\n",
      "[2025-12-23 11:59:41,317] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/adapter_config.json HTTP/1.1\" 307 0\n",
      "[2025-12-23 11:59:41,350] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e/adapter_config.json HTTP/1.1\" 200 0\n",
      "[2025-12-23 11:59:47,912] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/adapter_model.safetensors HTTP/1.1\" 404 0\n",
      "[2025-12-23 11:59:48,152] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/adapter_model.bin HTTP/1.1\" 302 0\n",
      "[2025-12-23 11:59:48,884] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/adapter_model.safetensors HTTP/1.1\" 404 0\n",
      "[2025-12-23 11:59:49,115] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/adapter_model.bin HTTP/1.1\" 302 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MMEBModel(\n",
       "  (encoder): PeftModel(\n",
       "    (base_model): LoraModel(\n",
       "      (model): Qwen2VLForConditionalGeneration(\n",
       "        (visual): Qwen2VisionTransformerPretrainedModel(\n",
       "          (patch_embed): PatchEmbed(\n",
       "            (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "          )\n",
       "          (rotary_pos_emb): VisionRotaryEmbedding()\n",
       "          (blocks): ModuleList(\n",
       "            (0-31): 32 x Qwen2VLVisionBlock(\n",
       "              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): VisionSdpaAttention(\n",
       "                (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "                (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (mlp): VisionMlp(\n",
       "                (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                (act): QuickGELUActivation()\n",
       "                (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (merger): PatchMerger(\n",
       "            (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=5120, out_features=1536, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (model): Qwen2VLModel(\n",
       "          (embed_tokens): Embedding(151936, 1536)\n",
       "          (layers): ModuleList(\n",
       "            (0-27): 28 x Qwen2VLDecoderLayer(\n",
       "              (self_attn): Qwen2VLSdpaAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1536, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1536, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 1536 (cuda:0)])\n",
       "                )\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1536, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 256 (cuda:0)])\n",
       "                )\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1536, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 256 (cuda:0)])\n",
       "                )\n",
       "                (o_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1536, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1536, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 1536 (cuda:0)])\n",
       "                )\n",
       "                (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): Qwen2MLP(\n",
       "                (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "                (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "                (down_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=8960, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1536, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 1536 (cuda:0)])\n",
       "                )\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "              (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "            )\n",
       "          )\n",
       "          (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "          (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "        )\n",
       "        (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cross_entropy): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args = ModelArguments(\n",
    "    model_name='Qwen/Qwen2-VL-2B-Instruct',\n",
    "    checkpoint_path='TIGER-Lab/VLM2Vec-Qwen2VL-2B',\n",
    "    pooling='last',\n",
    "    normalize=True,\n",
    "    model_backbone='qwen2_vl',\n",
    "    lora=True\n",
    ")\n",
    "data_args = DataArguments()\n",
    "\n",
    "processor = load_processor(model_args, data_args)\n",
    "model = MMEBModel.load(model_args)\n",
    "model = model.to('cuda', dtype=torch.bfloat16)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58c68afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2VisionTransformerPretrainedModel(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "  )\n",
       "  (rotary_pos_emb): VisionRotaryEmbedding()\n",
       "  (blocks): ModuleList(\n",
       "    (0-31): 32 x Qwen2VLVisionBlock(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): VisionSdpaAttention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp): VisionMlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): QuickGELUActivation()\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (merger): PatchMerger(\n",
       "    (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=5120, out_features=1536, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05e08652",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('assets/bus.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43a5b3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Image + Text -> Text\n",
    "inputs = processor(text=f'{VLM_IMAGE_TOKENS[QWEN2_VL]} Represent the given image with the following question: What is in the image',\n",
    "                   images=image.copy(),\n",
    "                   return_tensors=\"pt\")\n",
    "# inputs = processor(text=f'Represent the given image with the following question: What is in the image',\n",
    "#                    return_tensors=\"pt\")\n",
    "inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)\n",
    "inputs['image_grid_thw'] = inputs['image_grid_thw'].unsqueeze(0)\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b468b2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2VLProcessor:\n",
       "- image_processor: Qwen2VLImageProcessor {\n",
       "  \"do_convert_rgb\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.48145466,\n",
       "    0.4578275,\n",
       "    0.40821073\n",
       "  ],\n",
       "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.26862954,\n",
       "    0.26130258,\n",
       "    0.27577711\n",
       "  ],\n",
       "  \"max_pixels\": 1003520,\n",
       "  \"merge_size\": 2,\n",
       "  \"min_pixels\": 3136,\n",
       "  \"patch_size\": 14,\n",
       "  \"processor_class\": \"Qwen2VLProcessor\",\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"longest_edge\": 1003520,\n",
       "    \"shortest_edge\": 3136\n",
       "  },\n",
       "  \"temporal_patch_size\": 2\n",
       "}\n",
       "\n",
       "- tokenizer: Qwen2TokenizerFast(name_or_path='TIGER-Lab/VLM2Vec-Qwen2VL-2B', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")\n",
       "\n",
       "{\n",
       "  \"processor_class\": \"Qwen2VLProcessor\"\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee6b4566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(810, 1080)\n",
      "tensor([[[ 1, 78, 58]]], device='cuda:0')\n",
      "torch.Size([1, 4524, 1176])\n",
      "torch.Size([1, 1145])\n"
     ]
    }
   ],
   "source": [
    "print(image.size)\n",
    "print(inputs['image_grid_thw'])\n",
    "print(inputs['pixel_values'].shape)\n",
    "print(inputs['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4e82da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151655, 151655, 151655,  ...,    304,    279,   2168]],\n",
      "       device='cuda:0')\n",
      "151655\n",
      "tensor(1131, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(inputs['input_ids'])\n",
    "print(model.config.image_token_id)\n",
    "print((inputs['input_ids']==model.config.image_token_id).sum())\n",
    "\n",
    "attn = inputs[\"attention_mask\"]\n",
    "num_pad = (attn == 0).sum()\n",
    "print(num_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebdc1ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_resize(\n",
    "    height: int, width: int, factor: int = 28, min_pixels: int = 56 * 56, max_pixels: int = 14 * 14 * 4 * 1280\n",
    "):\n",
    "    \"\"\"Rescales the image so that the following conditions are met:\n",
    "    1. Both dimensions (height and width) are divisible by 'factor'.\n",
    "    2. The total number of pixels is within the range ['min_pixels', 'max_pixels'].\n",
    "    3. The aspect ratio of the image is maintained as closely as possible.\n",
    "    \"\"\"\n",
    "    if height < factor or width < factor or max(height, width) / min(height, width) > 200:\n",
    "        # extreme cases, resize to a square\n",
    "        height = width = max(factor, height, width)\n",
    "    h_bar = round(height / factor) * factor\n",
    "    w_bar = round(width / factor) * factor\n",
    "    if h_bar * w_bar > max_pixels:\n",
    "        beta = math.sqrt((height * width) / max_pixels)\n",
    "        h_bar = math.floor(height / beta / factor) * factor\n",
    "        w_bar = math.floor(width / beta / factor) * factor\n",
    "    elif h_bar * w_bar < min_pixels:\n",
    "        beta = math.sqrt(min_pixels / (height * width))\n",
    "        h_bar = math.ceil(height * beta / factor) * factor\n",
    "        w_bar = math.ceil(width * beta / factor) * factor\n",
    "    return h_bar, w_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93bf40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = processor.image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6995a589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 810, 3)\n"
     ]
    }
   ],
   "source": [
    "image = np.array(image)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c1f045a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original size: 1080 810\n",
      "patch size: 14\n",
      "merge size: 2\n",
      "temporal patch size: 2\n",
      "(1092, 812, 3)\n",
      "shape before patchify: (2, 3, 1092, 812)\n",
      "shape after patchify: (4524, 1176)\n"
     ]
    }
   ],
   "source": [
    "height, width = image.shape[0], image.shape[1]\n",
    "\n",
    "patch_size = image_processor.patch_size\n",
    "merge_size = image_processor.merge_size\n",
    "temporal_patch_size = image_processor.temporal_patch_size # 2\n",
    "size = image_processor.size\n",
    "\n",
    "print(\"original size:\", height, width)\n",
    "print(\"patch size:\", patch_size)\n",
    "print(\"merge size:\", merge_size)\n",
    "print(\"temporal patch size:\", temporal_patch_size)\n",
    "\n",
    "resized_height, resized_width = smart_resize(\n",
    "                    height,\n",
    "                    width,\n",
    "                    factor=patch_size * merge_size,\n",
    "                    min_pixels=size[\"shortest_edge\"],\n",
    "                    max_pixels=size[\"longest_edge\"],\n",
    "                )\n",
    "\n",
    "\n",
    "processed_image = resize(image, size=(resized_height, resized_width), \n",
    "               resample=image_processor.resample, \n",
    "               input_data_format=\"channels_last\")\n",
    "\n",
    "print(processed_image.shape)\n",
    "\n",
    "# rescale_factor = image_processor.rescale_factor\n",
    "# processed_image = image_processor.rescale(processed_image, \n",
    "#                                           scale=rescale_factor,\n",
    "#                                           input_data_format=\"channels_last\")\n",
    "\n",
    "# print(rescale_factor, processed_image.shape)\n",
    "\n",
    "\n",
    "patches = np.array([processed_image])\n",
    "patches = patches.transpose(0, 3, 1, 2)\n",
    "if patches.shape[0] % temporal_patch_size != 0:\n",
    "    repeats = np.repeat(\n",
    "        patches[-1][np.newaxis], temporal_patch_size - (patches.shape[0] % temporal_patch_size), axis=0\n",
    "    )\n",
    "    patches = np.concatenate([patches, repeats], axis=0)\n",
    "\n",
    "\n",
    "print(\"shape before patchify:\", patches.shape)\n",
    "\n",
    "channel = patches.shape[1]\n",
    "grid_t = patches.shape[0] // temporal_patch_size\n",
    "grid_h, grid_w = resized_height // patch_size, resized_width // patch_size\n",
    "patches = patches.reshape(\n",
    "    grid_t,\n",
    "    temporal_patch_size,\n",
    "    channel,\n",
    "    grid_h // merge_size,\n",
    "    merge_size,\n",
    "    patch_size,\n",
    "    grid_w // merge_size,\n",
    "    merge_size,\n",
    "    patch_size,\n",
    ")\n",
    "patches = patches.transpose(0, 3, 6, 4, 7, 2, 1, 5, 8)\n",
    "flatten_patches = patches.reshape(\n",
    "    grid_t * grid_h * grid_w, channel * temporal_patch_size * patch_size * patch_size\n",
    ")\n",
    "\n",
    "print(\"shape after patchify:\", flatten_patches.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2c8af45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2VLForConditionalGeneration(\n",
       "      (visual): Qwen2VisionTransformerPretrainedModel(\n",
       "        (patch_embed): PatchEmbed(\n",
       "          (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "        )\n",
       "        (rotary_pos_emb): VisionRotaryEmbedding()\n",
       "        (blocks): ModuleList(\n",
       "          (0-31): 32 x Qwen2VLVisionBlock(\n",
       "            (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): VisionSdpaAttention(\n",
       "              (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "              (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (mlp): VisionMlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): QuickGELUActivation()\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (merger): PatchMerger(\n",
       "          (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=5120, out_features=1536, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (model): Qwen2VLModel(\n",
       "        (embed_tokens): Embedding(151936, 1536)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x Qwen2VLDecoderLayer(\n",
       "            (self_attn): Qwen2VLSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 1536 (cuda:0)])\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 256 (cuda:0)])\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 256 (cuda:0)])\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 1536 (cuda:0)])\n",
       "              )\n",
       "              (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "              (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8960, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 1536 (cuda:0)])\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inference\n",
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21de545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-23 12:00:06,299] WARNING [src.model.vlm_backbone.qwen2_vl.modeling_qwen2_vl:329] Qwen2VLModel is using Qwen2VLSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values', 'hidden_states', 'attentions', 'rope_deltas'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# must comment _attn_implementation in src/model/model.py/MMEBModel.load/ to return outputs_attention\n",
    "\n",
    "hidden_states = model.encoder(**inputs, return_dict=True, output_hidden_states=True, output_attentions=True)\n",
    "hidden_states.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1afd85f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1145, 1536])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states['hidden_states'][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbe68a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 1145, 1145])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states['attentions'][-1].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
