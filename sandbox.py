import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig
from pprint import pprint  # ƒë·ªÉ in ƒë·∫πp h∆°n

MID = "apple/FastVLM-0.5B"

# H·∫±ng s·ªë m√¥ ph·ªèng v·ªã tr√≠ ·∫£nh
IMAGE_TOKEN_INDEX = -200  

# 1Ô∏è‚É£ Load tokenizer
tok = AutoTokenizer.from_pretrained(MID, trust_remote_code=True)

# 2Ô∏è‚É£ Load model
model = AutoModelForCausalLM.from_pretrained(
    MID,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto",
    trust_remote_code=True,
)

# 3Ô∏è‚É£ Load config
config = AutoConfig.from_pretrained(MID, trust_remote_code=True)

# 4Ô∏è‚É£ In th√¥ng tin t·ªïng quan
print("=" * 60)
print("‚úÖ MODEL INFORMATION")
print(f"Model class: {type(model)}")
print(f"Tokenizer class: {type(tok)}")
print(f"Model type: {config.model_type}")
print(f"Padding side: {getattr(config, 'padding_side', 'N/A')}")
print("=" * 60)

# 5Ô∏è‚É£ In c·∫•u h√¨nh chi ti·∫øt (r√∫t g·ªçn)
print("\nüß† CONFIG DETAILS (r√∫t g·ªçn):")
keys_to_show = [
    "model_type", "architectures", "torch_dtype",
    "vision_tower", "image_size", "patch_size",
    "text_config", "vision_config",
    "use_cache", "padding_side", "tie_word_embeddings"
]

for key in keys_to_show:
    value = getattr(config, key, None)
    if value is not None:
        print(f"- {key}: {value}")

# 6Ô∏è‚É£ N·∫øu b·∫°n mu·ªën xem to√†n b·ªô config (kh√¥ng r√∫t g·ªçn):
# print("\nFull config dump:")
# pprint(config.to_dict())
