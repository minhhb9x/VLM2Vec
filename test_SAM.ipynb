{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fdb6dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import SAM, YOLO\n",
    "from ultralytics.models.sam import SAM3SemanticPredictor\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dfe2dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-20 07:07:37--  https://www.modelscope.cn/models/facebook/sam3/resolve/master/sam3.pt\n",
      "Resolving www.modelscope.cn (www.modelscope.cn)... 47.251.62.57\n",
      "Connecting to www.modelscope.cn (www.modelscope.cn)|47.251.62.57|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-cn-1.modelscope.cn/prod/lfs-objects/99/99/e2341ceef5e136daa386eecb55cb414446a00ac2b55eb2dfd2f7c3cf8c9e?filename=sam3.pt&namespace=facebook&repository=sam3&revision=master&tag=model&auth_key=1766214458-cd7f3fc39e294b82bd301829136ebf06-0-ed7db393b8c3450d32c69d0818274ff6 [following]\n",
      "--2025-12-20 07:07:39--  https://cdn-lfs-cn-1.modelscope.cn/prod/lfs-objects/99/99/e2341ceef5e136daa386eecb55cb414446a00ac2b55eb2dfd2f7c3cf8c9e?filename=sam3.pt&namespace=facebook&repository=sam3&revision=master&tag=model&auth_key=1766214458-cd7f3fc39e294b82bd301829136ebf06-0-ed7db393b8c3450d32c69d0818274ff6\n",
      "Resolving cdn-lfs-cn-1.modelscope.cn (cdn-lfs-cn-1.modelscope.cn)... 47.246.38.177, 47.246.38.181, 47.246.38.220, ...\n",
      "Connecting to cdn-lfs-cn-1.modelscope.cn (cdn-lfs-cn-1.modelscope.cn)|47.246.38.177|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3450062241 (3.2G) [application/octet-stream]\n",
      "Saving to: ‚Äòsam3.pt‚Äô\n",
      "\n",
      "sam3.pt             100%[===================>]   3.21G  47.7MB/s    in 64s     \n",
      "\n",
      "2025-12-20 07:08:43 (51.6 MB/s) - ‚Äòsam3.pt‚Äô saved [3450062241/3450062241]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.modelscope.cn/models/facebook/sam3/resolve/master/sam3.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b41db09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 810, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread('assets/bus.jpg')\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6d2bf7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_overlapping_masks(result):\n",
    "    \"\"\"\n",
    "    H√†m l·ªçc b·ªè c√°c pixel ch·ªìng l·∫•n d·ª±a tr√™n confidence score.\n",
    "    Input: result object t·ª´ YOLO model.predict()\n",
    "    Output: Tensor masks m·ªõi (N, H, W) kh√¥ng c√≤n pixel ch·ªìng nhau.\n",
    "    \"\"\"\n",
    "    if result.masks is None:\n",
    "        return None\n",
    "\n",
    "    # 1. L·∫•y d·ªØ li·ªáu masks v√† confidence\n",
    "    # masks.data c√≥ d·∫°ng (N, H, W) - N l√† s·ªë object, H, W l√† k√≠ch th∆∞·ªõc mask\n",
    "    masks = result.masks.data \n",
    "    confs = result.boxes.conf  # Tensor (N,) ch·ª©a ƒëi·ªÉm confidence c·ªßa t·ª´ng box\n",
    "    \n",
    "    # L·∫•y k√≠ch th∆∞·ªõc\n",
    "    N, H, W = masks.shape\n",
    "    \n",
    "    if N == 0:\n",
    "        return masks\n",
    "\n",
    "    # 2. T·∫°o b·∫£n ƒë·ªì tr·ªçng s·ªë (Weighted Map)\n",
    "    # Reshape confs th√†nh (N, 1, 1) ƒë·ªÉ nh√¢n v·ªõi masks (Broadcasting)\n",
    "    # K·∫øt qu·∫£: Pixel n√†o thu·ªôc mask s·∫Ω c√≥ gi√° tr·ªã = confidence, kh√¥ng th√¨ = 0\n",
    "    confs_reshaped = confs.view(N, 1, 1)\n",
    "    weighted_masks = masks * confs_reshaped # (N, H, W)\n",
    "\n",
    "    # 3. T√¨m ch·ªâ s·ªë c·ªßa mask c√≥ ƒëi·ªÉm cao nh·∫•t t·∫°i m·ªói pixel\n",
    "    # max_vals: Gi√° tr·ªã conf l·ªõn nh·∫•t t·∫°i pixel ƒë√≥\n",
    "    # max_indices: Ch·ªâ s·ªë (index) c·ªßa mask s·ªü h·ªØu gi√° tr·ªã l·ªõn nh·∫•t ƒë√≥\n",
    "    max_vals, max_indices = torch.max(weighted_masks, dim=0) # (H, W)\n",
    "\n",
    "    # 4. T√°i t·∫°o l·∫°i masks kh√¥ng ch·ªìng l·∫•n (Clean Masks)\n",
    "    # T·∫°o m·ªôt tensor ch·ª©a c√°c index [0, 1, ..., N-1] ƒë·ªÉ so s√°nh\n",
    "    idx_range = torch.arange(N, device=masks.device).view(N, 1, 1)\n",
    "    \n",
    "    # Logic: M·ªôt pixel thu·ªôc v·ªÅ mask i KHI V√Ä CH·ªà KHI:\n",
    "    # - Mask i l√† mask chi·∫øn th·∫Øng t·∫°i pixel ƒë√≥ (max_indices == i)\n",
    "    # - V√† pixel ƒë√≥ th·ª±c s·ª± c√≥ d·ªØ li·ªáu (masks[i] > 0) - ƒë·ªÉ tr√°nh l·∫•y background\n",
    "    clean_masks = (max_indices.unsqueeze(0) == idx_range) & (masks > 0)\n",
    "\n",
    "    # Chuy·ªÉn v·ªÅ d·∫°ng float gi·ªëng mask g·ªëc\n",
    "    return clean_masks.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6b7f292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "overrides = dict(\n",
    "    conf=0.5,\n",
    "    task=\"segment\",\n",
    "    mode=\"predict\",\n",
    "    model=\"sam3.pt\",\n",
    "    half=True,  # Use FP16 for faster inference\n",
    "    save=True,\n",
    "    overlap_mask=False\n",
    ")\n",
    "\n",
    "model = SAM3SemanticPredictor(overrides=overrides) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "20d573de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ultralytics 8.3.240 üöÄ Python-3.11.10 torch-2.8.0+cu128 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "WARNING ‚ö†Ô∏è imgsz=[640] must be multiple of max stride 14, updating to [644]\n",
      "image 1/1 /workspace/VLM2Vec/assets/bus.jpg: 644x644 5 persons, 1 bus, 2 glassess, 130.4ms\n",
      "Speed: 2.4ms preprocess, 130.4ms inference, 0.8ms postprocess per image at shape (1, 3, 644, 644)\n",
      "Results saved to \u001b[1m/workspace/VLM2Vec/runs/segment/predict3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = model('assets/bus.jpg', text=[\"person\", \"bus\", \"glasses\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a2f68f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results_bus.jpg'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = results[0]\n",
    "clean_masks = clean_overlapping_masks(res)\n",
    "res.data = clean_masks  # C·∫≠p nh·∫≠t l·∫°i d·ªØ li·ªáu mask ƒë√£ l·ªçc\n",
    "res.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a9c1aa22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_masks = res.data\n",
    "clean_masks.sum(dim=0).max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
